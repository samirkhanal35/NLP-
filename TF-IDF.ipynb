{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da34df61",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba51df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef93b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best',\n",
    "                     'Germany beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c02529",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8578902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90991a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.89442719, 0.        , 0.4472136 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.70710678, 0.        , 0.        , 0.        ,\n",
       "        0.70710678],\n",
       "       [0.70710678, 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e802863e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'brazil', 'germany', 'love', 'sweden']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a2c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 4, 'brazil': 2, 'sweden': 5, 'best': 1, 'germany': 3, 'beats': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1866d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c47cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_space = pd.DataFrame(feature_matrix.toarray(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "806dbcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beats</th>\n",
       "      <th>best</th>\n",
       "      <th>brazil</th>\n",
       "      <th>germany</th>\n",
       "      <th>love</th>\n",
       "      <th>sweden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beats      best    brazil   germany      love    sweden\n",
       "0  0.000000  0.000000  0.894427  0.000000  0.447214  0.000000\n",
       "1  0.000000  0.707107  0.000000  0.000000  0.000000  0.707107\n",
       "2  0.707107  0.000000  0.000000  0.707107  0.000000  0.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352227c",
   "metadata": {},
   "source": [
    "## Text conversion to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53611c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = \"The 5 countries include China, United States, Indonesia, India, and Brazil.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13c4b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "str = myString.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4882f4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 5 countries include china, united states, indonesia, india, and brazil.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b00e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfdabaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the  countries include china united states indonesia india and brazil'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "puntuation_removed_str = re.sub(r'[!@#$%^&*()-+=,.\\d+]','', str)\n",
    "puntuation_removed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35feaec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'countries',\n",
       " 'include',\n",
       " 'china,',\n",
       " 'united',\n",
       " 'states,',\n",
       " 'indonesia,',\n",
       " 'india,',\n",
       " 'and',\n",
       " 'brazil.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_str = re.findall(r'\\w\\S+', str)\n",
    "filtered_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63282760",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_str = word_tokenize(puntuation_removed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be02a634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'countries',\n",
       " 'include',\n",
       " 'china',\n",
       " 'united',\n",
       " 'states',\n",
       " 'indonesia',\n",
       " 'india',\n",
       " 'and',\n",
       " 'brazil']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "930a60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = 'You, {]$%are amazing students:at@@! at @Lambton College ! ;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb7dc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "test_str = myString.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f499df43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are amazing studentsat at Lambton College  '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ead3199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "96850fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are amazing studentsat at Lambton College  '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuation_removed_str = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`\\{|\\}~]','', myString)\n",
    "puntuation_removed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b72023f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are amazing studentsat at Lambton College  '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuation_removed_str2 = re.sub(r'[^\\w\\s]', '', myString)\n",
    "puntuation_removed_str2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07125c",
   "metadata": {},
   "source": [
    "# POS - Tagging (Part Of Speech - Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c277d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19a3b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "15bc83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=(\"Codespeedy is a programming blog. \"\"Blog posts contain articles and tutorials on Python, CSS and even much more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fcd257b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Codespeedy is a programming blog. Blog posts contain articles and tutorials on Python, CSS and even much more'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8f55f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "814da91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Codespeedy', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('programming', 'VBG'), ('blog', 'NN'), ('Blog', 'NNP'), ('posts', 'NNS'), ('contain', 'VBP'), ('articles', 'NNS'), ('and', 'CC'), ('tutorials', 'NNS'), ('on', 'IN'), ('Python', 'NNP'), ('CSS', 'NNP'), ('and', 'CC'), ('even', 'RB'), ('much', 'RB'), ('more', 'JJR')]\n"
     ]
    }
   ],
   "source": [
    "print(tb.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4658c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'PRP'), ('are', 'VBP'), ('amazing', 'VBG'), ('studentsat', 'NN'), ('at', 'IN'), ('Lambton', 'NNP'), ('College', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tb1 = TextBlob(puntuation_removed_str2)\n",
    "print(tb1.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e6fd49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46b386",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5a3d2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = ' Jack Nelson worked for Microsoft and attended a conference in Italy . I Study at Lambton college in toronto.'\n",
    "myString2 = ' jack Nelson worked for Microsoft and attended a conference in Italy . I study at Lambton college in Toronto.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b5175024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9b60ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(myString)\n",
    "\n",
    "sentences2 = sent_tokenize(myString2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3f93a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "token_sentences2 = [word_tokenize(sent) for sent in sentences2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "58c465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "\n",
    "pos_sentences2 = [nltk.pos_tag(sent) for sent in token_sentences2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "795894d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Jack', 'NNP'),\n",
       "  ('Nelson', 'NNP'),\n",
       "  ('worked', 'VBD'),\n",
       "  ('for', 'IN'),\n",
       "  ('Microsoft', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('attended', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('conference', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('Italy', 'NNP'),\n",
       "  ('.', '.')],\n",
       " [('I', 'PRP'),\n",
       "  ('Study', 'VBP'),\n",
       "  ('at', 'IN'),\n",
       "  ('Lambton', 'NNP'),\n",
       "  ('college', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('toronto', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d26786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary = True)\n",
    "\n",
    "chunked_sentences2 = nltk.ne_chunk_sents(pos_sentences2, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "787c3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Jack/NNP Nelson/NNP)\n",
      "(NE Microsoft/NNP)\n",
      "(NE Italy/NNP)\n",
      "(NE Lambton/NNP)\n"
     ]
    }
   ],
   "source": [
    "# For myString\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == 'NE':\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eb54a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Nelson/NNP)\n",
      "(NE Microsoft/NNP)\n",
      "(NE Italy/NNP)\n",
      "(NE Lambton/NNP)\n",
      "(NE Toronto/NNP)\n"
     ]
    }
   ],
   "source": [
    "# For myString2\n",
    "for sent in chunked_sentences2:\n",
    "    for chunk in sent:\n",
    "        if isinstance(chunk, nltk.tree.Tree):\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32914808",
   "metadata": {},
   "source": [
    "## Synonym and antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ffeeadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5821b8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n"
     ]
    }
   ],
   "source": [
    "syn = wordnet.synsets(\"cat\")\n",
    "print(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a81d7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word:good\n",
      "Synonyms: {'upright', 'unspoilt', 'dependable', 'honorable', 'dear', 'effective', 'safe', 'good', 'respectable', 'adept', 'estimable', 'beneficial', 'skillful', 'near', 'soundly', 'salutary', 'sound', 'commodity', 'well', 'just', 'thoroughly', 'goodness', 'practiced', 'serious', 'full', 'secure', 'ripe', 'skilful', 'expert', 'in_force', 'trade_good', 'proficient', 'right', 'in_effect', 'honest', 'undecomposed', 'unspoiled'}\n",
      "Antonyms: {'evilness', 'evil', 'ill', 'bad', 'badness'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "input_word = input(\"Enter a word:\")\n",
    "\n",
    "for syn in wordnet.synsets(input_word):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(\"Synonyms:\", set(synonyms))\n",
    "print(\"Antonyms:\", set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "86aec75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "478c95e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('jump.n.02')\n",
    "w2 = wordnet.synset('spring.n.02')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8c8086a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('jump.v.02')\n",
    "w2 = wordnet.synset('spring.v.02')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "30a28212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('startle.v.02.startle'),\n",
       " Lemma('startle.v.02.jump'),\n",
       " Lemma('startle.v.02.start')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.v.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d2c92ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She startled when I walked into the room']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.v.02').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "541aad3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('leap.n.02.leap'),\n",
       " Lemma('leap.n.02.jump'),\n",
       " Lemma('leap.n.02.saltation')]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.n.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "30098569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a successful leap from college to the major leagues']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.n.02').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c7217cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a jump in attendance']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4662e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('jump.n.01.jump'), Lemma('jump.n.01.leap')]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('jump.n.01').lemmas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
